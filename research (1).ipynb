{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantBook Initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: QuantBook Initialization \n",
    "qb = QuantBook() # Assuming QuantBook is initialized in the environment\n",
    "print(\"QuantBook Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Updated Imports: Added Dropout\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# Re-adding QuantBook import explicitly for clarity within the script if run standalone\n",
    "from QuantConnect.Research import QuantBook\n",
    "print(\"Imports loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Function Definitions\n",
    "\n",
    "# Data Retrieval (Keep original function, ensures modularity)\n",
    "def get_training_data():\n",
    "    \"\"\"\n",
    "    Uses QuantConnect's QuantBook to fetch historical SPY data (Daily)\n",
    "    from 2000-01-01 to 2010-01-01.\n",
    "    Returns a DataFrame with 'open', 'high', 'low', 'close', 'volume'.\n",
    "    \"\"\"\n",
    "    # Ensure QuantBook is available and initialized\n",
    "    if 'QuantBook' not in globals():\n",
    "        raise EnvironmentError(\"QuantBook required but not initialized.\")\n",
    "    qb = QuantBook() # Re-initialize locally if needed or use global\n",
    "    spy_symbol = qb.AddEquity(\"SPY\", Resolution.Daily).Symbol\n",
    "    history = qb.History(\n",
    "        spy_symbol,\n",
    "        start=datetime(2000, 1, 1),\n",
    "        end=datetime(2010, 1, 1), # Training data end date\n",
    "        resolution=Resolution.Daily\n",
    "    )\n",
    "    if history.empty:\n",
    "        raise ValueError(\"Failed to fetch training data.\")\n",
    "\n",
    "    df = history.droplevel(0) # Assuming multi-index from History\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'time': 'date'}, inplace=True)\n",
    "    df.set_index('date', inplace=True)\n",
    "    # Select necessary columns, drop others if present\n",
    "    df = df[['open', 'high', 'low', 'close', 'volume']]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Feature Extraction (Refactored: Removed volume_changes, updated features, added comment)\n",
    "def extract_and_scale_features(df: pd.DataFrame, lookback: int):\n",
    "    \"\"\"\n",
    "    Extracts features (price changes, overnight gaps) and scales them globally.\n",
    "    Returns features array, targets array, and the fitted scaler.\n",
    "    Refactored: Removed 'volume_changes'.\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    targets = []\n",
    "\n",
    "    # Calculate raw features first\n",
    "    df['price_changes'] = df['close'] - df['open']\n",
    "    df['overnight_gaps'] = df['open'] - df['close'].shift(1)\n",
    "    # df['volume_changes'] = df['volume'] - df['volume'].shift(1) # Removed volume_changes\n",
    "\n",
    "    # Target: next day's price change (close - open)\n",
    "    df['target'] = (df['close'].shift(-1) - df['open'].shift(-1))\n",
    "\n",
    "    # Drop NaNs created by shifts/calculations (affects first row for gaps, last for target)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Updated feature columns list\n",
    "    feature_cols = ['price_changes', 'overnight_gaps']\n",
    "\n",
    "    # Scale the features globally\n",
    "    scaler = StandardScaler()\n",
    "    # WARNING: Global scaling assumes feature statistics (mean, std dev) are stationary over the entire training period.\n",
    "    # Concept drift can occur if statistics change significantly later.\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols]) # Fit and transform on training data\n",
    "\n",
    "    # Create sequences\n",
    "    for i in range(len(df) - lookback):\n",
    "        # Select only the specified feature_cols\n",
    "        window_features = df.iloc[i : i + lookback][feature_cols].values\n",
    "        target_value = df.iloc[i + lookback]['target'] # Target is after the window\n",
    "\n",
    "        # Ensure complete window and correct number of features\n",
    "        if window_features.shape[0] == lookback and window_features.shape[1] == len(feature_cols):\n",
    "            features_list.append(window_features)\n",
    "            targets.append(target_value)\n",
    "\n",
    "    if not features_list:\n",
    "        raise ValueError(\"Could not generate feature sequences.\")\n",
    "\n",
    "    # features shape: (num_samples, lookback, num_features=2)\n",
    "    # targets shape: (num_samples,)\n",
    "    return np.array(features_list), np.array(targets), scaler\n",
    "\n",
    "\n",
    "# RNN Pretraining Function (Refactored: hyperparameters, model structure, weight extraction indices)\n",
    "def pretrain_and_save_rnn_keras():\n",
    "    \"\"\"\n",
    "    Trains a refactored RNN using Keras and saves the fitted scaler and weights\n",
    "    to QuantConnect's ObjectStore.\n",
    "    Refactored: Simplified model, added Dropout, increased L2, reduced features.\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    df = get_training_data()\n",
    "\n",
    "    # Hyperparameters (Refactored)\n",
    "    lookback = 10           # RNN sequence length (should match main.py)\n",
    "    feature_count = 2       # Updated: Number of features used\n",
    "    hidden_size = 16        # Updated: Reduced complexity\n",
    "    learning_rate = 0.001   # Kept original\n",
    "    l2_lambda = 5e-4        # Updated: Increased regularization\n",
    "    epochs = 50             # Kept original\n",
    "    batch_size = 32         # Kept original\n",
    "    validation_split = 0.2  # Kept original\n",
    "    patience = 10           # Kept original\n",
    "\n",
    "    # Extract features, targets, and the FITTED scaler (using refactored function)\n",
    "    X, y, scaler = extract_and_scale_features(df, lookback)\n",
    "\n",
    "    # Ensure feature count matches\n",
    "    if X.shape[2] != feature_count:\n",
    "         raise ValueError(f\"Feature dimension mismatch: Expected {feature_count}, got {X.shape[2]}\")\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=validation_split, random_state=42, shuffle=False # Keep shuffle=False for time series\n",
    "    )\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}\")\n",
    "\n",
    "    # --- Keras Model Definition (Refactored) ---\n",
    "    model = Sequential([\n",
    "        Input(shape=(lookback, feature_count)), # Updated input shape\n",
    "        SimpleRNN(hidden_size,                # Updated hidden_size\n",
    "                  activation='tanh',\n",
    "                  kernel_regularizer=l2(l2_lambda),    # Updated l2_lambda\n",
    "                  recurrent_regularizer=l2(l2_lambda), # Updated l2_lambda\n",
    "                  bias_regularizer=l2(l2_lambda)),     # Updated l2_lambda\n",
    "        Dropout(0.25), # Added Dropout layer\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "    print(\"Starting Keras model training...\")\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=2)\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "   # Extract weights (Adjusted indices due to Dropout layer - Corrected)\n",
    "    best_weights = {}\n",
    "    # Layer indices: 0=SimpleRNN, 1=Dropout, 2=Dense\n",
    "    if len(model.layers) < 3: # Check if model has at least the expected layers\n",
    "         raise RuntimeError(f\"Model structure doesn't match expected layers after training. Found layers: {len(model.layers)}\")\n",
    "\n",
    "    # Corrected indices:\n",
    "    rnn_layer = model.layers[0]   # SimpleRNN is the first layer (index 0)\n",
    "    dense_layer = model.layers[2] # Dense is the third layer (index 2)\n",
    "\n",
    "    rnn_weights = rnn_layer.get_weights()\n",
    "    dense_weights = dense_layer.get_weights()\n",
    "\n",
    "    # Ensure weights are extracted correctly\n",
    "    if len(rnn_weights) != 3 or len(dense_weights) != 2:\n",
    "         raise RuntimeError(f\"Unexpected number of weight arrays retrieved. RNN: {len(rnn_weights)}, Dense: {len(dense_weights)}\")\n",
    "\n",
    "    # Transpose and reshape as expected by the original main.py's forward pass\n",
    "    best_weights[\"Wxh\"] = rnn_weights[0].T # Kernel (input-to-hidden)\n",
    "    best_weights[\"Whh\"] = rnn_weights[1]   # Recurrent Kernel (hidden-to-hidden)\n",
    "    best_weights[\"bh\"] = rnn_weights[2].reshape(-1, 1) # Bias (hidden)\n",
    "    best_weights[\"Why\"] = dense_weights[0].T # Kernel (hidden-to-output)\n",
    "    best_weights[\"by\"] = dense_weights[1].reshape(-1, 1) # Bias (output)\n",
    "    print(\"Weights extracted from trained model.\")\n",
    "\n",
    "    # --- Save to ObjectStore ---\n",
    "    if 'QuantBook' not in globals():\n",
    "        print(\"Error: QuantBook not available. Cannot save to ObjectStore.\")\n",
    "        return # Or raise error\n",
    "\n",
    "    qb = QuantBook() # Re-initialize locally or use global\n",
    "    scaler_key = \"rnn_strategy/scaler.pkl\"\n",
    "    weights_key = \"rnn_strategy/weights.pkl\"\n",
    "\n",
    "    # Save Scaler\n",
    "    try:\n",
    "        scaler_bytes = pickle.dumps(scaler)\n",
    "        success_scaler = qb.ObjectStore.SaveBytes(scaler_key, scaler_bytes)\n",
    "        if success_scaler:\n",
    "            print(f\"Scaler successfully saved to ObjectStore with key: '{scaler_key}'\")\n",
    "        else:\n",
    "            print(f\"Failed to save scaler to ObjectStore.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving scaler to ObjectStore: {e}\")\n",
    "\n",
    "    # Save Weights\n",
    "    try:\n",
    "        weights_bytes = pickle.dumps(best_weights)\n",
    "        success_weights = qb.ObjectStore.SaveBytes(weights_key, weights_bytes)\n",
    "        if success_weights:\n",
    "            print(f\"Weights successfully saved to ObjectStore with key: '{weights_key}'\")\n",
    "        else:\n",
    "            print(f\"Failed to save weights to ObjectStore.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving weights to ObjectStore: {e}\")\n",
    "\n",
    "print(\"All necessary functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RNN Pretraining and Saving at 2025-04-15 18:03:46.997540...\n",
      "Training data shape: (2002, 10, 2), Validation data shape: (501, 10, 2)\n",
      "Starting Keras model training...\n",
      "Epoch 1/50\n",
      "63/63 - 1s - 15ms/step - loss: 1.0470 - val_loss: 1.9239\n",
      "Epoch 2/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.8503 - val_loss: 1.7680\n",
      "Epoch 3/50\n",
      "63/63 - 0s - 3ms/step - loss: 0.7906 - val_loss: 1.6979\n",
      "Epoch 4/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7511 - val_loss: 1.6767\n",
      "Epoch 5/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7362 - val_loss: 1.6747\n",
      "Epoch 6/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7184 - val_loss: 1.6677\n",
      "Epoch 7/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7176 - val_loss: 1.6679\n",
      "Epoch 8/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7216 - val_loss: 1.6650\n",
      "Epoch 9/50\n",
      "63/63 - 0s - 3ms/step - loss: 0.7185 - val_loss: 1.6661\n",
      "Epoch 10/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7080 - val_loss: 1.6655\n",
      "Epoch 11/50\n",
      "63/63 - 0s - 3ms/step - loss: 0.7174 - val_loss: 1.6672\n",
      "Epoch 12/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7033 - val_loss: 1.6648\n",
      "Epoch 13/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7024 - val_loss: 1.6666\n",
      "Epoch 14/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.7056 - val_loss: 1.6681\n",
      "Epoch 15/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.6867 - val_loss: 1.6683\n",
      "Epoch 16/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.6988 - val_loss: 1.6684\n",
      "Epoch 17/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.6922 - val_loss: 1.6700\n",
      "Epoch 18/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.6871 - val_loss: 1.6686\n",
      "Epoch 19/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.6912 - val_loss: 1.6699\n",
      "Epoch 20/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.6853 - val_loss: 1.6679\n",
      "Epoch 21/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.6963 - val_loss: 1.6690\n",
      "Epoch 22/50\n",
      "63/63 - 0s - 2ms/step - loss: 0.6818 - val_loss: 1.6705\n",
      "Training finished.\n",
      "Weights extracted from trained model.\n",
      "Scaler successfully saved to ObjectStore with key: 'rnn_strategy/scaler.pkl'\n",
      "Weights successfully saved to ObjectStore with key: 'rnn_strategy/weights.pkl'\n",
      "RNN Pretraining and Saving finished at 2025-04-15 18:03:52.920477. Check logs above for status.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Execute the Pretraining and Saving Process (Keep original execution logic)\n",
    "print(f\"Starting RNN Pretraining and Saving at {datetime.now()}...\")\n",
    "try:\n",
    "    # Call the main function defined in the previous cell\n",
    "    pretrain_and_save_rnn_keras()\n",
    "    print(f\"RNN Pretraining and Saving finished at {datetime.now()}. Check logs above for status.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during pretraining execution: {e}\")\n",
    "    # You might want to print traceback for debugging\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Foundation-Py-Default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
