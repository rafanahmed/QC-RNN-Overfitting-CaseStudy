{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantBook Initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: QuantBook Initialization \n",
    "qb = QuantBook() # Assuming QuantBook is initialized in the environment\n",
    "print(\"QuantBook Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Updated Imports: Added Dropout\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# Re-adding QuantBook import explicitly for clarity within the script if run standalone\n",
    "from QuantConnect.Research import QuantBook\n",
    "print(\"Imports loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Walk-Forward Training Process ===\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2009-12-29 ---\n",
      "Fetching history from 2000-01-01 to 2009-12-29\n",
      "Processed 2512 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (2000, 10, 2), Validation data shape: (500, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20091229.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20091229.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2010-12-29 ---\n",
      "Fetching history from 2000-01-01 to 2010-12-29\n",
      "Processed 2764 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (2201, 10, 2), Validation data shape: (551, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20101229.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20101229.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2011-12-29 ---\n",
      "Fetching history from 2000-01-01 to 2011-12-29\n",
      "Processed 3017 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (2404, 10, 2), Validation data shape: (601, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20111229.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20111229.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2012-12-28 ---\n",
      "Fetching history from 2000-01-01 to 2012-12-28\n",
      "Processed 3267 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (2604, 10, 2), Validation data shape: (651, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20121228.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20121228.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2013-12-28 ---\n",
      "Fetching history from 2000-01-01 to 2013-12-28\n",
      "Processed 3519 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (2805, 10, 2), Validation data shape: (702, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20131228.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20131228.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2014-12-28 ---\n",
      "Fetching history from 2000-01-01 to 2014-12-28\n",
      "Processed 3770 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (3006, 10, 2), Validation data shape: (752, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20141228.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20141228.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2015-12-28 ---\n",
      "Fetching history from 2000-01-01 to 2015-12-28\n",
      "Processed 4021 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (3207, 10, 2), Validation data shape: (802, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20151228.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20151228.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2016-12-27 ---\n",
      "Fetching history from 2000-01-01 to 2016-12-27\n",
      "Processed 4273 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (3408, 10, 2), Validation data shape: (853, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20161227.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20161227.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2017-12-27 ---\n",
      "Fetching history from 2000-01-01 to 2017-12-27\n",
      "Processed 4525 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (3610, 10, 2), Validation data shape: (903, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20171227.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20171227.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2018-12-27 ---\n",
      "Fetching history from 2000-01-01 to 2018-12-27\n",
      "Processed 4776 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (3811, 10, 2), Validation data shape: (953, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20181227.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20181227.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2019-12-27 ---\n",
      "Fetching history from 2000-01-01 to 2019-12-27\n",
      "Processed 5028 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (4012, 10, 2), Validation data shape: (1004, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20191227.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20191227.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2020-12-26 ---\n",
      "Fetching history from 2000-01-01 to 2020-12-26\n",
      "Processed 5280 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (4214, 10, 2), Validation data shape: (1054, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20201226.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20201226.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2021-12-26 ---\n",
      "Fetching history from 2000-01-01 to 2021-12-26\n",
      "Processed 5531 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (4415, 10, 2), Validation data shape: (1104, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20211226.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20211226.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2022-12-26 ---\n",
      "Fetching history from 2000-01-01 to 2022-12-26\n",
      "Processed 5783 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (4616, 10, 2), Validation data shape: (1155, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20221226.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20221226.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2023-12-26 ---\n",
      "Fetching history from 2000-01-01 to 2023-12-26\n",
      "Processed 6033 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (4816, 10, 2), Validation data shape: (1205, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20231226.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20231226.pkl'\n",
      "\n",
      "--- Processing Training Period: 2000-01-01 to 2024-12-25 ---\n",
      "Fetching history from 2000-01-01 to 2024-12-25\n",
      "Processed 6285 rows.\n",
      "Fitting new scaler for the current window.\n",
      "Training data shape: (5018, 10, 2), Validation data shape: (1255, 10, 2)\n",
      "Starting Keras model training...\n",
      "Training finished for period.\n",
      "Weights extracted successfully.\n",
      "Scaler saved to ObjectStore: 'rnn_strategy/scaler_20241225.pkl'\n",
      "Weights saved to ObjectStore: 'rnn_strategy/weights_20241225.pkl'\n",
      "\n",
      "=== Walk-Forward Training Process Finished ===\n",
      "Total Training Periods Attempted: 16\n",
      "Successful Periods Completed: 16\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Function Definitions (Modified get_data_for_period)\n",
    "\n",
    "# MODIFIED: Parameterized get_training_data (renamed for clarity)\n",
    "#           Added robust DataFrame processing for columns/index\n",
    "def get_data_for_period(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Uses QuantConnect's QuantBook to fetch historical SPY data (Daily)\n",
    "    for the SPECIFIED period.\n",
    "    Returns a DataFrame with 'open', 'high', 'low', 'close', 'volume'.\n",
    "    \"\"\"\n",
    "    # Ensure QuantBook is available and initialized\n",
    "    if 'QuantBook' not in globals():\n",
    "        # Attempt initialization if not found\n",
    "        try:\n",
    "            from QuantConnect import QuantBook\n",
    "            global qb\n",
    "            qb = QuantBook()\n",
    "            print(\"QuantBook initialized within get_data_for_period.\")\n",
    "        except ImportError:\n",
    "             raise EnvironmentError(\"QuantBook required but not initialized and cannot import.\")\n",
    "\n",
    "    spy_symbol = qb.AddEquity(\"SPY\", Resolution.Daily).Symbol\n",
    "\n",
    "    print(f\"Fetching history from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        history = qb.History(\n",
    "            spy_symbol,\n",
    "            start=start_date, # Use passed start_date\n",
    "            end=end_date,     # Use passed end_date\n",
    "            resolution=Resolution.Daily\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during qb.History call: {e}\")\n",
    "        return pd.DataFrame() # Return empty DataFrame on error\n",
    "\n",
    "    if history.empty:\n",
    "        print(f\"Warning: No history data returned for SPY between {start_date} and {end_date}.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame explicitly\n",
    "\n",
    "    # --- Robust DataFrame Processing ---\n",
    "    # Check if index is MultiIndex (Symbol, Time)\n",
    "    if isinstance(history.index, pd.MultiIndex):\n",
    "        # If MultiIndex, drop the 'symbol' level to make time the primary index\n",
    "        df = history.droplevel(0)\n",
    "    else:\n",
    "        # If single index (Time), just use the history directly\n",
    "        df = history\n",
    "\n",
    "    # Check if COLUMNS are MultiIndex (less common for single symbol, but possible)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "         # If columns are MultiIndex, assume top level is symbol, keep second level\n",
    "         df.columns = df.columns.get_level_values(1)\n",
    "\n",
    "    # Ensure standard column names exist before selecting\n",
    "    required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: DataFrame missing required columns. Found: {df.columns.tolist()}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Select only the required columns\n",
    "    df = df[required_cols].copy()\n",
    "\n",
    "    # Ensure index is DatetimeIndex or similar, convert to date if needed\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = df.index.date\n",
    "    elif isinstance(df.index, pd.Index) and pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "         df.index = pd.to_datetime(df.index).date\n",
    "    # If index is already date objects, do nothing\n",
    "\n",
    "    print(f\"Processed {len(df)} rows.\")\n",
    "    return df\n",
    "\n",
    "# MODIFIED: extract_and_scale_features now fits scaler ONLY on current window data\n",
    "def extract_and_scale_features(df: pd.DataFrame, lookback: int):\n",
    "    \"\"\"\n",
    "    Extracts features, target, creates sequences, and scales features.\n",
    "    Fits a NEW scaler on the provided DataFrame (current training window).\n",
    "    Returns features array, targets array, and the FITTED scaler.\n",
    "    \"\"\"\n",
    "    if df.empty or len(df) <= lookback + 1:\n",
    "         print(f\"Insufficient data for feature extraction (requires > {lookback + 1} days, got {len(df)}).\")\n",
    "         return None, None, None # Return None for X, y, scaler\n",
    "\n",
    "    features_list = []\n",
    "    targets = []\n",
    "\n",
    "    # Calculate raw features first\n",
    "    df = df.copy() # Work on a copy to avoid modifying original slice\n",
    "    df['price_changes'] = df['close'] - df['open']\n",
    "    df['overnight_gaps'] = df['open'] - df['close'].shift(1)\n",
    "\n",
    "    # Target: next day's price change (close - open)\n",
    "    df['next_open'] = df['open'].shift(-1)\n",
    "    df['next_close'] = df['close'].shift(-1)\n",
    "    df['target'] = df['next_close'] - df['next_open']\n",
    "\n",
    "    # Drop NaNs created by shifts/calculations\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    if len(df) < lookback:\n",
    "        print(f\"Insufficient data after NaN drop for sequence creation (requires {lookback} days, got {len(df)}).\")\n",
    "        return None, None, None\n",
    "\n",
    "    feature_cols = ['price_changes', 'overnight_gaps']\n",
    "    # Ensure feature columns exist after calculations and dropna\n",
    "    if not all(col in df.columns for col in feature_cols):\n",
    "        print(f\"Error: Missing feature columns after calculations. Found: {df.columns.tolist()}\")\n",
    "        return None, None, None\n",
    "\n",
    "    features = df[feature_cols].values\n",
    "    target_values = df['target'].values\n",
    "\n",
    "    # --- Scale the features for THIS window ---\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Fitting new scaler for the current window.\")\n",
    "    scaled_features = scaler.fit_transform(features) # Fit and transform ONLY on current window\n",
    "\n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_features) - lookback):\n",
    "        X.append(scaled_features[i:(i + lookback)])\n",
    "        y.append(target_values[i + lookback])\n",
    "\n",
    "    if not X:\n",
    "        print(\"Could not generate feature sequences.\")\n",
    "        return None, None, scaler # Return scaler even if no sequences\n",
    "\n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "\n",
    "# --- Assume build_rnn_model and extract_numpy_weights are defined as before ---\n",
    "def build_rnn_model(input_shape, hidden_size=16, dropout_rate=0.25, l2_reg=0.01):\n",
    "    # (Implementation as provided previously)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.SimpleRNN(hidden_size, activation='tanh',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                  recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                  bias_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                  name='simple_rnn'),\n",
    "        tf.keras.layers.Dropout(dropout_rate, name='dropout'),\n",
    "        tf.keras.layers.Dense(1, name='dense_output')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')\n",
    "    return model\n",
    "\n",
    "def extract_numpy_weights(keras_model):\n",
    "     # (Implementation as provided previously)\n",
    "    weights = {}\n",
    "    try:\n",
    "        rnn_layer = keras_model.get_layer('simple_rnn')\n",
    "        dense_layer = keras_model.get_layer('dense_output')\n",
    "        rnn_weights = rnn_layer.get_weights()\n",
    "        dense_weights = dense_layer.get_weights()\n",
    "        weights['Wxh'] = rnn_weights[0]\n",
    "        weights['Whh'] = rnn_weights[1]\n",
    "        weights['bh'] = rnn_weights[2]\n",
    "        weights['Why'] = dense_weights[0]\n",
    "        weights['by'] = dense_weights[1]\n",
    "        print(\"Weights extracted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting weights: {e}\")\n",
    "        return None\n",
    "    return weights\n",
    "\n",
    "# Cell 4: Walk-Forward Training Execution Loop\n",
    "\n",
    "# --- Walk-Forward Configuration ---\n",
    "wf_config = {\n",
    "    \"initial_train_start_date\": datetime(2000, 1, 1),\n",
    "    \"overall_end_date\": datetime(2025, 3, 5), # Corresponds to algo end date\n",
    "    \"initial_training_years\": 10,\n",
    "    \"retrain_frequency_days\": 365, # Retrain approximately annually\n",
    "}\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "model_config = {\n",
    "    \"lookback\": 10,\n",
    "    \"feature_count\": 2,\n",
    "    \"hidden_size\": 16,\n",
    "    \"dropout_rate\": 0.25,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 32,\n",
    "    \"early_stopping_patience\": 10\n",
    "}\n",
    "\n",
    "# --- Execution Loop ---\n",
    "print(f\"\\n=== Starting Walk-Forward Training Process ===\")\n",
    "# Ensure qb is initialized before the loop if not done globally\n",
    "if 'QuantBook' not in globals():\n",
    "    try:\n",
    "        from QuantConnect import QuantBook\n",
    "        qb = QuantBook()\n",
    "        print(\"QuantBook initialized before loop.\")\n",
    "    except ImportError:\n",
    "         print(\"ERROR: QuantBook cannot be initialized. Stopping.\")\n",
    "         # Or raise an error depending on desired behavior\n",
    "         exit() # Exit if QB is essential and cannot be loaded\n",
    "\n",
    "\n",
    "current_train_end_date = wf_config[\"initial_train_start_date\"] + timedelta(days=wf_config[\"initial_training_years\"] * 365)\n",
    "successful_periods = 0\n",
    "total_periods = 0\n",
    "\n",
    "while current_train_end_date <= wf_config[\"overall_end_date\"]:\n",
    "    total_periods += 1\n",
    "    # Define training window for this iteration (Anchored)\n",
    "    train_start = wf_config[\"initial_train_start_date\"]\n",
    "    train_end = min(current_train_end_date, wf_config[\"overall_end_date\"]) # Don't go past overall end\n",
    "\n",
    "    print(f\"\\n--- Processing Training Period: {train_start.strftime('%Y-%m-%d')} to {train_end.strftime('%Y-%m-%d')} ---\")\n",
    "\n",
    "    # 1. Get Data for THIS period using the modified function\n",
    "    df_period = get_data_for_period(train_start, train_end)\n",
    "    if df_period.empty:\n",
    "        print(\"Skipping period due to lack of data.\")\n",
    "        current_train_end_date += timedelta(days=wf_config[\"retrain_frequency_days\"])\n",
    "        continue # Move to next period\n",
    "\n",
    "    # 2. Extract Features and Scale (Scaler is fitted here)\n",
    "    X_period, y_period, scaler_period = extract_and_scale_features(df_period, model_config['lookback'])\n",
    "    if X_period is None or y_period is None or scaler_period is None:\n",
    "        print(\"Skipping period due to feature extraction/scaling failure.\")\n",
    "        current_train_end_date += timedelta(days=wf_config[\"retrain_frequency_days\"])\n",
    "        continue\n",
    "\n",
    "    # 3. Train/Validation Split\n",
    "    try:\n",
    "        # Ensure sufficient data for split\n",
    "        if len(X_period) < 5: # Need at least a few samples for train/val\n",
    "             print(f\"Skipping period: Insufficient samples ({len(X_period)}) for train/validation split.\")\n",
    "             current_train_end_date += timedelta(days=wf_config[\"retrain_frequency_days\"])\n",
    "             continue\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_period, y_period, test_size=0.2, shuffle=False)\n",
    "        print(f\"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}\")\n",
    "        # Add check for empty validation set which can cause issues\n",
    "        if X_val.shape[0] == 0:\n",
    "             print(\"Warning: Validation set is empty after split. Early stopping may not work as expected.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "         print(f\"Error during train/validation split (likely insufficient data): {e}\")\n",
    "         current_train_end_date += timedelta(days=wf_config[\"retrain_frequency_days\"])\n",
    "         continue\n",
    "\n",
    "    # 4. Build and Train Model\n",
    "    try:\n",
    "        model = build_rnn_model(input_shape=(model_config['lookback'], model_config['feature_count']),\n",
    "                                  hidden_size=model_config['hidden_size'],\n",
    "                                  dropout_rate=model_config['dropout_rate'],\n",
    "                                  l2_reg=model_config['l2_reg'])\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                           patience=model_config['early_stopping_patience'],\n",
    "                                                           restore_best_weights=True)\n",
    "        print(\"Starting Keras model training...\")\n",
    "        # Avoid issues if validation set was empty\n",
    "        callbacks_list = [early_stopping] if X_val.shape[0] > 0 else []\n",
    "        validation_data_tuple = (X_val, y_val) if X_val.shape[0] > 0 else None\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  epochs=model_config['epochs'],\n",
    "                  batch_size=model_config['batch_size'],\n",
    "                  validation_data=validation_data_tuple,\n",
    "                  callbacks=callbacks_list,\n",
    "                  verbose=0) # Use verbose=1 or 2 for detailed logs\n",
    "        print(\"Training finished for period.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model building or training: {e}\")\n",
    "        current_train_end_date += timedelta(days=wf_config[\"retrain_frequency_days\"])\n",
    "        continue\n",
    "\n",
    "    # 5. Extract Weights\n",
    "    numpy_weights_period = extract_numpy_weights(model)\n",
    "    if numpy_weights_period is None:\n",
    "        print(\"Skipping period due to weight extraction failure.\")\n",
    "        current_train_end_date += timedelta(days=wf_config[\"retrain_frequency_days\"])\n",
    "        continue\n",
    "\n",
    "    # 6. Save Scaler and Weights with Period-Specific Keys\n",
    "    period_suffix = train_end.strftime('%Y%m%d') # Use the END date of training\n",
    "    scaler_key = f\"rnn_strategy/scaler_{period_suffix}.pkl\"\n",
    "    weights_key = f\"rnn_strategy/weights_{period_suffix}.pkl\"\n",
    "\n",
    "    try:\n",
    "        scaler_bytes = pickle.dumps(scaler_period)\n",
    "        if qb.ObjectStore.SaveBytes(scaler_key, scaler_bytes):\n",
    "            print(f\"Scaler saved to ObjectStore: '{scaler_key}'\")\n",
    "        else:\n",
    "            print(f\"Error: Failed to save scaler to ObjectStore: '{scaler_key}'\")\n",
    "            # Decide how to handle partial failure (maybe skip saving weights too)\n",
    "\n",
    "        weights_bytes = pickle.dumps(numpy_weights_period)\n",
    "        if qb.ObjectStore.SaveBytes(weights_key, weights_bytes):\n",
    "            print(f\"Weights saved to ObjectStore: '{weights_key}'\")\n",
    "            successful_periods += 1 # Count success only if both save\n",
    "        else:\n",
    "            print(f\"Error: Failed to save weights to ObjectStore: '{weights_key}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error pickling or saving scaler/weights: {e}\")\n",
    "\n",
    "    # Move to the next training period end date\n",
    "    current_train_end_date += timedelta(days=wf_config[\"retrain_frequency_days\"])\n",
    "\n",
    "    # Break if the last processed period already reached the overall end date\n",
    "    if train_end == wf_config[\"overall_end_date\"]:\n",
    "         break\n",
    "\n",
    "\n",
    "print(f\"\\n=== Walk-Forward Training Process Finished ===\")\n",
    "print(f\"Total Training Periods Attempted: {total_periods}\")\n",
    "print(f\"Successful Periods Completed: {successful_periods}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Foundation-Py-Default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
